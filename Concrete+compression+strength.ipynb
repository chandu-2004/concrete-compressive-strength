{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/concrete-compressive-strength-data-set/concrete_data.csv')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data does not have any missing values. Let's see the distribution of the data ","metadata":{"editable":false}},{"cell_type":"code","source":"for column in df.columns:\n    plt.figure(figsize=(14,4))\n    sns.boxplot(x=column,data=df)\n    plt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset doesn't have that much outliers","metadata":{"editable":false}},{"cell_type":"code","source":"sns.pairplot(data=df,diag_kind='kde')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features of the datasets are not normally distributed, so there is a need to make them normally distributed. We will make use of qq plots","metadata":{"editable":false}},{"cell_type":"markdown","source":"Before, making the data noprmally distributed let's divide the whole dataset into X and y","metadata":{"editable":false}},{"cell_type":"code","source":"df.columns","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df.drop('concrete_compressive_strength',axis=1)\ny=df['concrete_compressive_strength']","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transforming the data into normal distribution using box-cox transformer","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\npt=PowerTransformer()\nX_normal_transformed_arr=pt.fit_transform(X)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_normal_transformed=pd.DataFrame(X_normal_transformed_arr,columns=X.columns)\nfrom scipy import stats","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in X.columns:\n    plt.figure(figsize=(16,5))\n    plt.subplot(2,2,1)\n    stats.probplot(x=X[col],dist='norm',plot=plt)\n    plt.title(col)\n    plt.subplot(2,2,2)\n    stats.probplot(x=X_normal_transformed[col],dist='norm',plot=plt)\n    plt.title(label=col+' After Transformation')\n    plt.subplot(2,2,3)\n    sns.kdeplot(x=X[col])\n    plt.subplot(2,2,4)\n    sns.kdeplot(x=X_normal_transformed[col])\n    plt.show()\n    \n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X_normal_transformed_arr,y,test_size=1/3)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=LinearRegression()\nlr.fit(X_train,y_train)\nlr.score(X_test,y_test)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=lr.predict(X_test)\nlr_pred_df=pd.DataFrame(np.c_[y_test,y_pred],columns=['Actual compressive strength','Predicted compressive strength by LR'])\nlr_pred_df","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr=RandomForestRegressor()\nrfr.fit(X_train,y_train)\nrfr.score(X_test,y_test)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_range=[i for i in range(15,1000)]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" As compared to linear regression, Random Forest Regressor is more efficient.Let's try to make it more efficient by grid search cross validation technique and oob_score","metadata":{"editable":false}},{"cell_type":"code","source":"param_dist={'max_depth':[2,3,4,5],\n           'bootstrap':[True,False],\n           'max_features':['auto','sqrt','log2',None],\n            }\ncv_rfr=GridSearchCV(rfr,cv=10,param_grid=param_dist,n_jobs=-1)\ncv_rfr.fit(X_train,y_train)\nprint(cv_rfr.best_params_)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr.set_params(max_depth=5,bootstrap=True,max_features='auto')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr.score(X_train,y_train)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr.score(X_test,y_test)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, random forest regressor is better as compared to Linear Regression.Let's try the performance of adboost regressor on this dataset and which performs better","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nadbr=AdaBoostRegressor(n_estimators=1500,learning_rate=0.1,loss='square')\nadbr.fit(X_train,y_train)\nprint(adbr.score(X_train,y_train))\nprint(adbr.score(X_test,y_test))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, the accuracy of adaboost gradient regressor is less than Random Forest Regressor. Let's try gradient boost regressor.","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\ngrbst=GradientBoostingRegressor(max_depth=2,warm_start=True)\nmin_val_error=float('inf')\nerror_going_up=0\nfor n_estimators in range(1,1500):\n    grbst.n_estimators=n_estimators\n    grbst.fit(X_train,y_train)\n    y_pred=grbst.predict(X_test)\n    error=mean_squared_error(y_test,y_pred)\n    if error<min_val_error:\n        min_val_error=error\n        error_going_up=0\n    else:\n        error_going_up+=1\n        if error_going_up==5:\n            break","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grbst.n_estimators","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grbst.score(X_train,y_train))\nprint(grbst.score(X_test,y_test))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see that the performance of the gradient boosting regressor is similar to the RandomForestRegressor","metadata":{"editable":false}},{"cell_type":"markdown","source":"Let's try Extreme gradient Regressor","metadata":{"editable":false}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgbr=XGBRegressor()\nxgbr.fit(X_train,y_train)\nprint(xgbr.score(X_train,y_train))\nprint(xgbr.score(X_test,y_test))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence, the maximum accuracy is being given XGboost regressor. So, I would deploy the model based on this algorithm only.","metadata":{"editable":false}}]}